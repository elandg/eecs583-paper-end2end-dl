{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Demonstration\n",
    "\n",
    "### Source Rewriter\n",
    "\n",
    "Given the following example code (taken from Nvidiaâ€™s streamcluster benchmark):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "//#define Elements\n",
      "__kernel void memset_kernel(__global char * mem_d, short val, int number_bytes){\n",
      "    const int thread_id = get_global_id(0);\n",
      "    mem_d[thread_id] = val;\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "code = \"\"\"//#define Elements\n",
    "__kernel void memset_kernel(__global char * mem_d, short val, int number_bytes){\n",
    "    const int thread_id = get_global_id(0);\n",
    "    mem_d[thread_id] = val;\n",
    "}\"\"\"\n",
    "\n",
    "print(code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply the rewriter. Variable and function names are normalized, comments removed, and code style enforced:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elandg/eecs583-paper-end2end-dl/env/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/elandg/eecs583-paper-end2end-dl/env/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/elandg/eecs583-paper-end2end-dl/env/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/elandg/eecs583-paper-end2end-dl/env/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/elandg/eecs583-paper-end2end-dl/env/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/elandg/eecs583-paper-end2end-dl/env/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/elandg/eecs583-paper-end2end-dl/env/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/elandg/eecs583-paper-end2end-dl/env/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/elandg/eecs583-paper-end2end-dl/env/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/elandg/eecs583-paper-end2end-dl/env/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/elandg/eecs583-paper-end2end-dl/env/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/elandg/eecs583-paper-end2end-dl/env/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__kernel void A(__global char* a, short b, int c) {\n",
      "  const int d = get_global_id(0);\n",
      "  a[d] = b;\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from clgen import preprocess\n",
    "\n",
    "rewritten = preprocess(code)\n",
    "print(rewritten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source Encoder\n",
    "\n",
    "Deriving a 1-of-$k$ vocabulary for a piece of code, using a hybrid character and token based approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GreedyAtomizer[27 tokens]\n"
     ]
    }
   ],
   "source": [
    "from clgen._atomizer import GreedyAtomizer\n",
    "from clgen._langs import Language\n",
    "\n",
    "\n",
    "atomizer = GreedyAtomizer.from_text(lang=Language.from_str(\"opencl\"), text=rewritten)\n",
    "print(atomizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The derived vocabulary maps tokens to indices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>'\\n'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>'  '</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>' '</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>'('</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>')'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>'*'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>','</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>'0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>';'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>'='</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>'A'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>'['</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>']'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>'__global'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>'__kernel'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>'a'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>'b'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>'c'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>'char'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>'const'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>'d'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>'get_global_id'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>'int'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>'short'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>'void'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>'{'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>'}'</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              token\n",
       "0              '\\n'\n",
       "1              '  '\n",
       "2               ' '\n",
       "3               '('\n",
       "4               ')'\n",
       "5               '*'\n",
       "6               ','\n",
       "7               '0'\n",
       "8               ';'\n",
       "9               '='\n",
       "10              'A'\n",
       "11              '['\n",
       "12              ']'\n",
       "13       '__global'\n",
       "14       '__kernel'\n",
       "15              'a'\n",
       "16              'b'\n",
       "17              'c'\n",
       "18           'char'\n",
       "19          'const'\n",
       "20              'd'\n",
       "21  'get_global_id'\n",
       "22            'int'\n",
       "23          'short'\n",
       "24           'void'\n",
       "25              '{'\n",
       "26              '}'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(sorted([f\"'{k}'\" for k in atomizer.vocab]), columns=[\"token\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding the source using this vocabulary yields:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14  1 24  1 10  3 13  1 18  5  1 15  6  1 23  1 16  6  1 22  1 17  4  1\n",
      " 25  0  2 19  1 22  1 20  1  9  1 21  3  7  4  8  0  2 15 11 20 12  1  9\n",
      "  1 16  8  0 26]\n"
     ]
    }
   ],
   "source": [
    "encoded = atomizer.atomize(rewritten)\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reversing the process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__kernel>< ><void>< ><A><(><__global>< ><char><*>< ><a><,>< ><short>< ><b><,>< ><int>< ><c><)>< ><{><\\n><  ><const>< ><int>< ><d>< ><=>< ><get_global_id><(><0><)><;><\\n><  ><a><[><d><]>< ><=>< ><b><;><\\n><}>"
     ]
    }
   ],
   "source": [
    "for i in encoded:\n",
    "    t = atomizer.deatomize([i])\n",
    "    if t == '\\n': t = '\\\\n'\n",
    "    print(f\"<{t}>\", end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding\n",
    "\n",
    "Sequences are padded to a fixed length using an out-of-vocabulary token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[27 27 27 27 27 27 27 27 27 27 27 27 27 27 27 27 27 27 27 27 27 27 14  1\n",
      " 24  1 10  3 13  1 18  5  1 15  6  1 23  1 16  6  1 22  1 17  4  1 25  0\n",
      "  2 19  1 22  1 20  1  9  1 21  3  7  4  8  0  2 15 11 20 12  1  9  1 16\n",
      "  8  0 26]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "pad_val = atomizer.vocab_size\n",
    "print(pad_sequences([encoded], maxlen=len(encoded) + 22, value=pad_val)[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_srcs(srcs):\n",
    "    \"\"\" encode and pad source code for learning \"\"\"\n",
    "    from keras.preprocessing.sequence import pad_sequences\n",
    "    \n",
    "    # take rewritten and turn to tokens\n",
    "    seqs = [atomizer.atomize(src) for src in srcs]\n",
    "    # get pad val\n",
    "    pad_val = atomizer.vocab_size\n",
    "    # add padding to match expected input size for model\n",
    "    encoded = np.array(pad_sequences(seqs, maxlen=1024, value=pad_val))\n",
    "    return np.vstack([np.expand_dims(x, axis=0) for x in encoded])\n",
    "\n",
    "def grewe_features(df: pd.DataFrame) -> np.array:\n",
    "    \"\"\" extract Grewe et al. feature vector from runtime data \"\"\"\n",
    "    return np.array([\n",
    "            (df[\"transfer\"].values / (df[\"comp\"].values + df[\"mem\"].values)),  # F1\n",
    "            (df[\"coalesced\"].values / df[\"mem\"].values),  # F2\n",
    "            ((df[\"localmem\"].values / df[\"mem\"].values) * df[\"wgsize\"].values),  # F3\n",
    "            (df[\"comp\"].values / df[\"mem\"].values),  # F4\n",
    "        ]).T\n",
    "\n",
    "def auxiliary_inputs(df: pd.DataFrame) -> np.array:\n",
    "    \"\"\" get dsize and wgsize auxiliary inputs \"\"\"\n",
    "    return np.array([\n",
    "        df[\"transfer\"].values,\n",
    "        df[\"wgsize\"].values,\n",
    "    ]).T\n",
    "\n",
    "from typing import List\n",
    "\n",
    "def platform2str(platform: str) -> str:\n",
    "    \"\"\" get full platform name \"\"\"\n",
    "    if platform == \"amd\":\n",
    "        return \"AMD Tahiti 7970\"\n",
    "    elif platform == \"nvidia\":\n",
    "        return \"NVIDIA GTX 970\"\n",
    "    else:\n",
    "        raise LookupException\n",
    "\n",
    "def encode_1hot(y: np.array) -> np.array:\n",
    "    \"\"\" 1-hot encode labels \"\"\"\n",
    "    labels = np.vstack([np.expand_dims(x, axis=0) for x in y])\n",
    "    l2 = [x[0] for x in labels]\n",
    "    l1 = [not x for x in l2]\n",
    "    return np.array(list(zip(l1, l2)), dtype=np.int32)\n",
    "\n",
    "def escape_benchmark_name(g: str) -> str:\n",
    "    \"\"\"escape benchmark name for display\"\"\"\n",
    "    c = g.split('-')\n",
    "    return escape_suite_name(c[0]).split()[0] + \".\" + c[-2]\n",
    "\n",
    "def escape_suite_name(g: str) -> str:\n",
    "    \"\"\" format benchmark suite name for display \"\"\"\n",
    "    c = g.split('-')\n",
    "    if c[0] == \"amd\" or c[0] == \"nvidia\":\n",
    "        return c[0].upper() + \" SDK\"\n",
    "    if c[0] == \"npb\" or c[0] == \"shoc\":\n",
    "        return c[0].upper()\n",
    "    elif c[0] == \"parboil\" or c[0] == \"polybench\" or c[0] == \"rodinia\":\n",
    "        return c[0].capitalize()\n",
    "    else:\n",
    "        raise LookupError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from clgen import _atomizer as clgen\n",
    "\n",
    "class HeterogemeousMappingModel(object):\n",
    "    \"\"\"\n",
    "    A model for predicting OpenCL heterogeneous device mappings.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    __name__ : str\n",
    "        Model name\n",
    "    __basename__ : str\n",
    "        Shortened name, used for files\n",
    "    \"\"\"\n",
    "    __name__ = None\n",
    "    __basename__ = None\n",
    "    \n",
    "    def init(self, seed: int) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the model.\n",
    "        \n",
    "        Do whatever is required to setup a new heterogeneous model here.\n",
    "        This method is called prior to training and predicting.\n",
    "        This method may be omitted if no initial setup is required.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        seed : int\n",
    "            The seed value used to reproducible results. May be 'None',\n",
    "            indicating that no seed is to be used.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def save(self, outpath: str) -> None:\n",
    "        \"\"\"\n",
    "        Save model state.\n",
    "        \n",
    "        This must capture all of the relevant state of the model. It is up\n",
    "        to implementing classes to determine how best to save the model.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        outpath : str\n",
    "            The path to save the model state to.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def restore(self, inpath: str) -> None:\n",
    "        \"\"\"\n",
    "        Load a trained model from file.\n",
    "        \n",
    "        This is called in place of init() if a saved model file exists. It\n",
    "        must restore all of the required model state.\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        inpath : str\n",
    "            The path to load the model from. This is the same path as\n",
    "            was passed to save() to create the file.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def train(self, df: pd.DataFrame, features: np.array, sequences: np.array,\n",
    "              y: np.array, y_1hot: np.array, verbose: bool=False) -> None:\n",
    "        \"\"\"\n",
    "        Train a model.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        df : pd.DataFrame\n",
    "            The platform dataframe.\n",
    "        \n",
    "        features : np.array\n",
    "            An array of feature vectors of shape (n,4).\n",
    "        \n",
    "        sequences : np.array\n",
    "            An array of encoded source code sequences of shape (n,seq_length).\n",
    "\n",
    "        y : np.array\n",
    "            An array of optimal device mappings of shape (n,1).\n",
    "        \n",
    "        y_1hot : np.array\n",
    "            An array of optimal device mappings of shape (n,2), in 1-hot encoding.\n",
    "            \n",
    "        verbose: bool, optional\n",
    "            Whether to print verbose status messages during training.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def predict(self, features: np.array, sequences: np.array, y: np.array,\n",
    "                y_1hot: np.array, verbose: bool=False) -> np.array:\n",
    "        \"\"\"\n",
    "        Make predictions for programs.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        features : np.array\n",
    "            An array of feature vectors of shape (n,4).\n",
    "        \n",
    "        sequences : np.array\n",
    "            An array of encoded source code sequences of shape (n,seq_length).\n",
    "\n",
    "        y : np.array\n",
    "            An array of optimal device mappings of shape (n,1).\n",
    "        \n",
    "        y_1hot : np.array\n",
    "            An array of optimal device mappings of shape (n,2), in 1-hot encoding.\n",
    "            \n",
    "        verbose: bool, optional\n",
    "            Whether to print verbose status messages.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        np.array\n",
    "            Predicted 'y' values (optimal device mappings) with shape (n,1).\n",
    "        \"\"\"\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepTune(HeterogemeousMappingModel):\n",
    "    __name__ = \"DeepTune\"\n",
    "    __basename__ = \"deeptune\"\n",
    "    \n",
    "    def init(self, seed: int):\n",
    "        from keras.layers import Input, Embedding, LSTM, Dense, Conv1D, GlobalMaxPooling1D\n",
    "        from keras.layers.merge import Concatenate\n",
    "        from keras.layers.normalization import BatchNormalization\n",
    "        from keras.models import Model\n",
    "\n",
    "        np.random.seed(seed)\n",
    "        \n",
    "        # Language model. Takes as inputs source code sequences.\n",
    "        code_in = Input(shape=(1024,), dtype=\"int32\", name=\"code_in\")\n",
    "        x = Embedding(input_dim=atomizer.vocab_size + 1, input_length=1024, \n",
    "                      output_dim=64, name=\"embedding\")(code_in)\n",
    "        # x = LSTM(64, implementation=1, return_sequences=True, name=\"lstm_1\")(x)\n",
    "        # x = LSTM(64, implementation=1, name=\"lstm_2\")(x)\n",
    "        x = Conv1D(32, 9, name=\"conv_1\")(x)\n",
    "        x = Conv1D(32, 9, name=\"conv_2\")(x)\n",
    "        # x = Conv1D(32, 9, name=\"conv_3\")(x)\n",
    "        x = GlobalMaxPooling1D()(x)\n",
    "        langmodel_out = Dense(2, activation=\"sigmoid\")(x)\n",
    "        \n",
    "        # Auxiliary inputs. wgsize and dsize.\n",
    "        auxiliary_inputs = Input(shape=(2,))\n",
    "        \n",
    "        # Heuristic model. Takes as inputs the language model,\n",
    "        #   outputs 1-hot encoded device mapping\n",
    "        x = Concatenate()([auxiliary_inputs, x])\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dense(32, activation=\"relu\")(x)\n",
    "        out = Dense(2, activation=\"sigmoid\")(x)\n",
    "\n",
    "        self.model = Model(inputs=[auxiliary_inputs, code_in], outputs=[out, langmodel_out])\n",
    "        self.model.compile(\n",
    "            optimizer=\"adam\", metrics=['accuracy'],\n",
    "            loss=[\"categorical_crossentropy\", \"categorical_crossentropy\"],\n",
    "            loss_weights=[1., .2])\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def save(self, outpath):\n",
    "        self.model.save(outpath)\n",
    "\n",
    "    def restore(self, inpath):\n",
    "        from keras.models import load_model\n",
    "        self.model = load_model(inpath)\n",
    "        \n",
    "    def train(self, **train):\n",
    "        self.model.fit([train[\"aux_in\"], train[\"sequences\"]], [train[\"y_1hot\"], train[\"y_1hot\"]],\n",
    "                       epochs=50, batch_size=64, verbose=train[\"verbose\"], shuffle=True)\n",
    "\n",
    "    def predict(self, **test):\n",
    "        p = np.array(self.model.predict(\n",
    "            [test[\"aux_in\"], test[\"sequences\"]], batch_size=64, verbose=test[\"verbose\"]))\n",
    "        indices = [np.argmax(x) for x in p[0]]\n",
    "        return indices\n",
    "\n",
    "    def predict_demo(self, **test):\n",
    "        p = np.array(self.model.predict(\n",
    "            [test[\"aux_in\"], test[\"sequences\"]], batch_size=1, verbose=test[\"verbose\"]))\n",
    "        indices = [np.argmax(x) for x in p[0]]\n",
    "        return indices"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GreedyAtomizer[128 tokens]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "srcs = '\\n'.join(pd.read_csv(\"../data/case-study-a/cgo17-amd.csv\")['src'].values)\n",
    "atomizer = clgen.GreedyAtomizer.from_text(lang=Language.from_str(\"opencl\"), text=srcs)\n",
    "atomizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/elandg/eecs583-paper-end2end-dl/env/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:492: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/elandg/eecs583-paper-end2end-dl/env/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3630: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/elandg/eecs583-paper-end2end-dl/env/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:63: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/elandg/eecs583-paper-end2end-dl/env/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:1208: calling reduce_max_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /home/elandg/eecs583-paper-end2end-dl/env/lib/python3.7/site-packages/keras/optimizers.py:711: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/elandg/eecs583-paper-end2end-dl/env/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:2880: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /home/elandg/eecs583-paper-end2end-dl/env/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:2884: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "seed = 204\n",
    "deeptune_model = DeepTune()\n",
    "deeptune_model.init(seed);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from labm8 import fs\n",
    "import pickle\n",
    "\n",
    "def evaluate(model: HeterogemeousMappingModel) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Evaluate a model.\n",
    "    \n",
    "    Performs 10-fold cross-validation of the model's effectiveness at predicting\n",
    "    OpenCL device mappings. Results are cached.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : HeterogeneousMappingModel\n",
    "        The predictive model to evaluate.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pd.Dataframe\n",
    "        Evaluation results.\n",
    "    \"\"\"\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from progressbar import ProgressBar\n",
    "    \n",
    "    progressbar = [0, ProgressBar(max_value=10*2)]\n",
    "\n",
    "    data = []\n",
    "    for i, platform in enumerate([\"amd\", \"nvidia\"]):\n",
    "        platform_name = platform2str(platform)\n",
    "\n",
    "        # load runtime data\n",
    "        df = pd.read_csv(f\"../data/case-study-a/cgo17-{platform}.csv\")\n",
    "        \n",
    "        sequences = None  # defer sequence encoding until needed (it's expensive)\n",
    "        \n",
    "        # values used for training & predictions\n",
    "        features = grewe_features(df)\n",
    "        aux_in = auxiliary_inputs(df)\n",
    "        \n",
    "        # optimal mappings\n",
    "        y = np.array([1 if x == \"GPU\" else 0 for x in df[\"oracle\"].values])\n",
    "        y_1hot = encode_1hot(y)\n",
    "        \n",
    "        # 10-fold cross-validation\n",
    "        # kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "        # for j, (train_index, test_index) in enumerate(kf.split(features, y)):\n",
    "        indices = np.arange(len(features))\n",
    "        train_index, test_index = train_test_split(indices, shuffle=True, random_state=seed, test_size=0.25)\n",
    "\n",
    "        model_path = f\"../data/case-study-a/models/{model.__basename__}-{platform}-demo.model\"\n",
    "        predictions_path = f\"../data/case-study-a/predictions/{model.__basename__}-{platform}-demo.result\"\n",
    "        \n",
    "        if False:\n",
    "        # if fs.exists(predictions_path):\n",
    "            # load result from cache\n",
    "            with open(predictions_path, 'rb') as infile:\n",
    "                p = pickle.load(infile)\n",
    "        else:\n",
    "            if sequences is None:  # encode source codes if needed\n",
    "                sequences = encode_srcs(df[\"src\"].values)\n",
    "            \n",
    "            if False:\n",
    "            # if fs.exists(model_path):\n",
    "                # restore trained model from cache\n",
    "                model.restore(model_path)\n",
    "            else:\n",
    "                # train and cache a model\n",
    "                model.init(seed=seed)                   \n",
    "                model.train(df=df,\n",
    "                            features=features[train_index],\n",
    "                            aux_in=aux_in[train_index],\n",
    "                            sequences=sequences[train_index],\n",
    "                            y=y[train_index],\n",
    "                            y_1hot=y_1hot[train_index],\n",
    "                            verbose=True)\n",
    "                fs.mkdir(fs.dirname(model_path))\n",
    "                model.save(model_path)\n",
    "\n",
    "            # test model\n",
    "            p = model.predict(\n",
    "                features=features[test_index],\n",
    "                aux_in=aux_in[test_index],\n",
    "                sequences=sequences[test_index],\n",
    "                y=y[test_index],\n",
    "                y_1hot=y_1hot[test_index],\n",
    "                verbose=True)\n",
    "\n",
    "        # cache results\n",
    "        fs.mkdir(fs.dirname(predictions_path))\n",
    "        with open(predictions_path, 'wb') as outfile:\n",
    "            pickle.dump(p, outfile)\n",
    "\n",
    "        # benchmarks\n",
    "        benchmarks = df['benchmark'].values[test_index]\n",
    "        # oracle device mappings\n",
    "        o = y[test_index]\n",
    "        # whether predictions were correct or not\n",
    "        correct = p == o\n",
    "        # runtimes of baseline mapping (CPU on AMD, GPU on NVIDIA)\n",
    "        zero_r_dev = \"runtime_cpu\" if platform == \"amd\" else \"runtime_gpu\"\n",
    "        zer_r_runtimes = df[zero_r_dev][test_index]\n",
    "        # speedups of predictions\n",
    "        runtimes = df[['runtime_cpu', 'runtime_gpu']].values[test_index]\n",
    "        p_runtimes = [r[p_] for p_, r in zip(p, runtimes)]\n",
    "        p_speedup = zer_r_runtimes / p_runtimes\n",
    "\n",
    "        # sanity check\n",
    "        assert(len(benchmarks) == len(o) == len(correct) == len(p) == len(p_speedup))\n",
    "\n",
    "        # record results\n",
    "        for benchmark_, o_, p_, correct_, p_speedup_ in zip(benchmarks, o, p, correct, p_speedup):\n",
    "            data.append({\n",
    "                \"Model\": model.__name__,\n",
    "                \"Platform\": platform_name,\n",
    "                'Benchmark': escape_benchmark_name(benchmark_),\n",
    "                'Benchmark Suite': escape_suite_name(benchmark_),\n",
    "                \"Oracle Mapping\": o_,\n",
    "                \"Predicted Mapping\": p_,\n",
    "                \"Correct?\": correct_,\n",
    "                \"Speedup\": p_speedup_,\n",
    "            })\n",
    "        \n",
    "        # update progress bar\n",
    "        progressbar[0] += 1\n",
    "        progressbar[1].update(progressbar[0])\n",
    "\n",
    "    return pd.DataFrame(\n",
    "        data, index=range(1, len(data)+1), columns=[\n",
    "            \"Model\",\n",
    "            \"Platform\",\n",
    "            \"Benchmark\",\n",
    "            \"Benchmark Suite\", \n",
    "            \"Oracle Mapping\", \n",
    "            \"Predicted Mapping\", \n",
    "            \"Correct?\", \n",
    "            \"Speedup\"\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating DeepTune ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/elandg/eecs583-paper-end2end-dl/env/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/elandg/eecs583-paper-end2end-dl/env/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:953: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/elandg/eecs583-paper-end2end-dl/env/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:675: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-14 14:18:33.048664: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2022-12-14 14:18:33.070232: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2808000000 Hz\n",
      "2022-12-14 14:18:33.077378: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x66c4a00 executing computations on platform Host. Devices:\n",
      "2022-12-14 14:18:33.077545: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
      "2022-12-14 14:18:34.271499: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "510/510 [==============================] - 7s 14ms/step - loss: 0.8038 - dense_6_loss: 0.6670 - dense_4_loss: 0.6838 - dense_6_acc: 0.5725 - dense_4_acc: 0.6000\n",
      "Epoch 2/50\n",
      "510/510 [==============================] - 4s 9ms/step - loss: 0.6899 - dense_6_loss: 0.5545 - dense_4_loss: 0.6770 - dense_6_acc: 0.7392 - dense_4_acc: 0.6000\n",
      "Epoch 3/50\n",
      "510/510 [==============================] - 4s 8ms/step - loss: 0.6206 - dense_6_loss: 0.4863 - dense_4_loss: 0.6717 - dense_6_acc: 0.7980 - dense_4_acc: 0.6000\n",
      "Epoch 4/50\n",
      "510/510 [==============================] - 4s 8ms/step - loss: 0.5673 - dense_6_loss: 0.4335 - dense_4_loss: 0.6694 - dense_6_acc: 0.8196 - dense_4_acc: 0.6000\n",
      "Epoch 5/50\n",
      "510/510 [==============================] - 4s 8ms/step - loss: 0.5300 - dense_6_loss: 0.3963 - dense_4_loss: 0.6689 - dense_6_acc: 0.8314 - dense_4_acc: 0.6000\n",
      "Epoch 6/50\n",
      "510/510 [==============================] - 4s 8ms/step - loss: 0.5020 - dense_6_loss: 0.3685 - dense_4_loss: 0.6676 - dense_6_acc: 0.8294 - dense_4_acc: 0.6000\n",
      "Epoch 7/50\n",
      "510/510 [==============================] - 4s 9ms/step - loss: 0.4662 - dense_6_loss: 0.3328 - dense_4_loss: 0.6667 - dense_6_acc: 0.8549 - dense_4_acc: 0.6000\n",
      "Epoch 8/50\n",
      "510/510 [==============================] - 4s 8ms/step - loss: 0.4519 - dense_6_loss: 0.3187 - dense_4_loss: 0.6658 - dense_6_acc: 0.8529 - dense_4_acc: 0.6000\n",
      "Epoch 9/50\n",
      "510/510 [==============================] - 4s 9ms/step - loss: 0.4303 - dense_6_loss: 0.2972 - dense_4_loss: 0.6655 - dense_6_acc: 0.8627 - dense_4_acc: 0.6000\n",
      "Epoch 10/50\n",
      "510/510 [==============================] - 4s 8ms/step - loss: 0.4295 - dense_6_loss: 0.2966 - dense_4_loss: 0.6647 - dense_6_acc: 0.8725 - dense_4_acc: 0.6000\n",
      "Epoch 11/50\n",
      "510/510 [==============================] - 4s 8ms/step - loss: 0.4122 - dense_6_loss: 0.2793 - dense_4_loss: 0.6644 - dense_6_acc: 0.8647 - dense_4_acc: 0.6000\n",
      "Epoch 12/50\n",
      "510/510 [==============================] - 4s 9ms/step - loss: 0.4101 - dense_6_loss: 0.2773 - dense_4_loss: 0.6640 - dense_6_acc: 0.8647 - dense_4_acc: 0.6000\n",
      "Epoch 13/50\n",
      "510/510 [==============================] - 5s 9ms/step - loss: 0.3987 - dense_6_loss: 0.2662 - dense_4_loss: 0.6626 - dense_6_acc: 0.8804 - dense_4_acc: 0.6000\n",
      "Epoch 14/50\n",
      "510/510 [==============================] - 4s 8ms/step - loss: 0.3892 - dense_6_loss: 0.2570 - dense_4_loss: 0.6611 - dense_6_acc: 0.8961 - dense_4_acc: 0.6000\n",
      "Epoch 15/50\n",
      "510/510 [==============================] - 5s 10ms/step - loss: 0.3895 - dense_6_loss: 0.2574 - dense_4_loss: 0.6603 - dense_6_acc: 0.8706 - dense_4_acc: 0.6000\n",
      "Epoch 16/50\n",
      "510/510 [==============================] - 7s 13ms/step - loss: 0.3755 - dense_6_loss: 0.2436 - dense_4_loss: 0.6596 - dense_6_acc: 0.8765 - dense_4_acc: 0.6000\n",
      "Epoch 17/50\n",
      "510/510 [==============================] - 6s 12ms/step - loss: 0.3720 - dense_6_loss: 0.2402 - dense_4_loss: 0.6586 - dense_6_acc: 0.8882 - dense_4_acc: 0.6000\n",
      "Epoch 18/50\n",
      "510/510 [==============================] - 5s 11ms/step - loss: 0.3600 - dense_6_loss: 0.2286 - dense_4_loss: 0.6571 - dense_6_acc: 0.8941 - dense_4_acc: 0.6000\n",
      "Epoch 19/50\n",
      "510/510 [==============================] - 5s 10ms/step - loss: 0.3597 - dense_6_loss: 0.2285 - dense_4_loss: 0.6559 - dense_6_acc: 0.8882 - dense_4_acc: 0.6000\n",
      "Epoch 20/50\n",
      "510/510 [==============================] - 5s 10ms/step - loss: 0.3530 - dense_6_loss: 0.2219 - dense_4_loss: 0.6552 - dense_6_acc: 0.8902 - dense_4_acc: 0.6000\n",
      "Epoch 21/50\n",
      "510/510 [==============================] - 5s 10ms/step - loss: 0.3490 - dense_6_loss: 0.2183 - dense_4_loss: 0.6537 - dense_6_acc: 0.8843 - dense_4_acc: 0.6000\n",
      "Epoch 22/50\n",
      "510/510 [==============================] - 5s 10ms/step - loss: 0.3420 - dense_6_loss: 0.2116 - dense_4_loss: 0.6518 - dense_6_acc: 0.9039 - dense_4_acc: 0.6000\n",
      "Epoch 23/50\n",
      "510/510 [==============================] - 5s 9ms/step - loss: 0.3463 - dense_6_loss: 0.2163 - dense_4_loss: 0.6501 - dense_6_acc: 0.8882 - dense_4_acc: 0.6000\n",
      "Epoch 24/50\n",
      "510/510 [==============================] - 4s 9ms/step - loss: 0.3304 - dense_6_loss: 0.2008 - dense_4_loss: 0.6483 - dense_6_acc: 0.9078 - dense_4_acc: 0.6000\n",
      "Epoch 25/50\n",
      "510/510 [==============================] - 5s 9ms/step - loss: 0.3237 - dense_6_loss: 0.1944 - dense_4_loss: 0.6467 - dense_6_acc: 0.9059 - dense_4_acc: 0.6000\n",
      "Epoch 26/50\n",
      "510/510 [==============================] - 5s 10ms/step - loss: 0.3206 - dense_6_loss: 0.1918 - dense_4_loss: 0.6440 - dense_6_acc: 0.9059 - dense_4_acc: 0.6000\n",
      "Epoch 27/50\n",
      "510/510 [==============================] - 5s 10ms/step - loss: 0.3193 - dense_6_loss: 0.1908 - dense_4_loss: 0.6425 - dense_6_acc: 0.9216 - dense_4_acc: 0.6000\n",
      "Epoch 28/50\n",
      "510/510 [==============================] - 5s 9ms/step - loss: 0.3072 - dense_6_loss: 0.1793 - dense_4_loss: 0.6393 - dense_6_acc: 0.9275 - dense_4_acc: 0.6000\n",
      "Epoch 29/50\n",
      "510/510 [==============================] - 4s 8ms/step - loss: 0.3031 - dense_6_loss: 0.1759 - dense_4_loss: 0.6363 - dense_6_acc: 0.9157 - dense_4_acc: 0.6000\n",
      "Epoch 30/50\n",
      "510/510 [==============================] - 4s 8ms/step - loss: 0.2915 - dense_6_loss: 0.1646 - dense_4_loss: 0.6345 - dense_6_acc: 0.9412 - dense_4_acc: 0.6000\n",
      "Epoch 31/50\n",
      "510/510 [==============================] - 4s 9ms/step - loss: 0.2877 - dense_6_loss: 0.1615 - dense_4_loss: 0.6313 - dense_6_acc: 0.9431 - dense_4_acc: 0.6000\n",
      "Epoch 32/50\n",
      "510/510 [==============================] - 5s 9ms/step - loss: 0.2955 - dense_6_loss: 0.1697 - dense_4_loss: 0.6287 - dense_6_acc: 0.9216 - dense_4_acc: 0.6000\n",
      "Epoch 33/50\n",
      "510/510 [==============================] - 4s 8ms/step - loss: 0.2851 - dense_6_loss: 0.1601 - dense_4_loss: 0.6250 - dense_6_acc: 0.9314 - dense_4_acc: 0.6020\n",
      "Epoch 34/50\n",
      "510/510 [==============================] - 4s 9ms/step - loss: 0.2876 - dense_6_loss: 0.1632 - dense_4_loss: 0.6221 - dense_6_acc: 0.9176 - dense_4_acc: 0.6000\n",
      "Epoch 35/50\n",
      "510/510 [==============================] - 4s 8ms/step - loss: 0.2811 - dense_6_loss: 0.1572 - dense_4_loss: 0.6195 - dense_6_acc: 0.9451 - dense_4_acc: 0.6000\n",
      "Epoch 36/50\n",
      "510/510 [==============================] - 4s 8ms/step - loss: 0.2759 - dense_6_loss: 0.1527 - dense_4_loss: 0.6160 - dense_6_acc: 0.9353 - dense_4_acc: 0.6039\n",
      "Epoch 37/50\n",
      "510/510 [==============================] - 4s 8ms/step - loss: 0.2596 - dense_6_loss: 0.1370 - dense_4_loss: 0.6131 - dense_6_acc: 0.9510 - dense_4_acc: 0.6176\n",
      "Epoch 38/50\n",
      "510/510 [==============================] - 5s 9ms/step - loss: 0.2565 - dense_6_loss: 0.1347 - dense_4_loss: 0.6091 - dense_6_acc: 0.9549 - dense_4_acc: 0.6510\n",
      "Epoch 39/50\n",
      "510/510 [==============================] - 4s 9ms/step - loss: 0.2634 - dense_6_loss: 0.1427 - dense_4_loss: 0.6036 - dense_6_acc: 0.9353 - dense_4_acc: 0.6686\n",
      "Epoch 40/50\n",
      "510/510 [==============================] - 4s 8ms/step - loss: 0.2595 - dense_6_loss: 0.1393 - dense_4_loss: 0.6009 - dense_6_acc: 0.9431 - dense_4_acc: 0.6529\n",
      "Epoch 41/50\n",
      "510/510 [==============================] - 4s 9ms/step - loss: 0.2665 - dense_6_loss: 0.1471 - dense_4_loss: 0.5971 - dense_6_acc: 0.9392 - dense_4_acc: 0.6706\n",
      "Epoch 42/50\n",
      "510/510 [==============================] - 5s 10ms/step - loss: 0.2630 - dense_6_loss: 0.1451 - dense_4_loss: 0.5897 - dense_6_acc: 0.9333 - dense_4_acc: 0.6882\n",
      "Epoch 43/50\n",
      "510/510 [==============================] - 4s 9ms/step - loss: 0.2554 - dense_6_loss: 0.1385 - dense_4_loss: 0.5845 - dense_6_acc: 0.9451 - dense_4_acc: 0.7000\n",
      "Epoch 44/50\n",
      "510/510 [==============================] - 4s 8ms/step - loss: 0.2516 - dense_6_loss: 0.1354 - dense_4_loss: 0.5813 - dense_6_acc: 0.9510 - dense_4_acc: 0.7176\n",
      "Epoch 45/50\n",
      "510/510 [==============================] - 5s 9ms/step - loss: 0.2441 - dense_6_loss: 0.1291 - dense_4_loss: 0.5749 - dense_6_acc: 0.9510 - dense_4_acc: 0.7137\n",
      "Epoch 46/50\n",
      "510/510 [==============================] - 4s 8ms/step - loss: 0.2467 - dense_6_loss: 0.1326 - dense_4_loss: 0.5705 - dense_6_acc: 0.9451 - dense_4_acc: 0.7196\n",
      "Epoch 47/50\n",
      "510/510 [==============================] - 4s 8ms/step - loss: 0.2340 - dense_6_loss: 0.1212 - dense_4_loss: 0.5640 - dense_6_acc: 0.9529 - dense_4_acc: 0.7451\n",
      "Epoch 48/50\n",
      "510/510 [==============================] - 4s 8ms/step - loss: 0.2272 - dense_6_loss: 0.1150 - dense_4_loss: 0.5610 - dense_6_acc: 0.9529 - dense_4_acc: 0.7176\n",
      "Epoch 49/50\n",
      "510/510 [==============================] - 4s 8ms/step - loss: 0.2227 - dense_6_loss: 0.1122 - dense_4_loss: 0.5527 - dense_6_acc: 0.9627 - dense_4_acc: 0.7510\n",
      "Epoch 50/50\n",
      "510/510 [==============================] - 4s 8ms/step - loss: 0.2273 - dense_6_loss: 0.1177 - dense_4_loss: 0.5483 - dense_6_acc: 0.9608 - dense_4_acc: 0.7725\n",
      "170/170 [==============================] - 1s 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0% (0 of 20) |                         | Elapsed Time: 0:00:00 ETA:  --:--:--"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "510/510 [==============================] - 7s 14ms/step - loss: 0.7473 - dense_9_loss: 0.6087 - dense_7_loss: 0.6931 - dense_9_acc: 0.6667 - dense_7_acc: 0.5235\n",
      "Epoch 2/50\n",
      "510/510 [==============================] - 4s 8ms/step - loss: 0.6025 - dense_9_loss: 0.4655 - dense_7_loss: 0.6851 - dense_9_acc: 0.8255 - dense_7_acc: 0.5627\n",
      "Epoch 3/50\n",
      "510/510 [==============================] - 4s 8ms/step - loss: 0.5285 - dense_9_loss: 0.3925 - dense_7_loss: 0.6801 - dense_9_acc: 0.8510 - dense_7_acc: 0.5627\n",
      "Epoch 4/50\n",
      "510/510 [==============================] - 4s 8ms/step - loss: 0.4787 - dense_9_loss: 0.3432 - dense_7_loss: 0.6775 - dense_9_acc: 0.8725 - dense_7_acc: 0.5627\n",
      "Epoch 5/50\n",
      "510/510 [==============================] - 4s 8ms/step - loss: 0.4456 - dense_9_loss: 0.3104 - dense_7_loss: 0.6755 - dense_9_acc: 0.8882 - dense_7_acc: 0.5627\n",
      "Epoch 6/50\n",
      "510/510 [==============================] - 5s 9ms/step - loss: 0.4074 - dense_9_loss: 0.2727 - dense_7_loss: 0.6738 - dense_9_acc: 0.8922 - dense_7_acc: 0.5627\n",
      "Epoch 7/50\n",
      "510/510 [==============================] - 4s 8ms/step - loss: 0.3858 - dense_9_loss: 0.2515 - dense_7_loss: 0.6717 - dense_9_acc: 0.9020 - dense_7_acc: 0.5627\n",
      "Epoch 8/50\n",
      "510/510 [==============================] - 4s 8ms/step - loss: 0.3634 - dense_9_loss: 0.2296 - dense_7_loss: 0.6691 - dense_9_acc: 0.9039 - dense_7_acc: 0.5627\n",
      "Epoch 9/50\n",
      "510/510 [==============================] - 4s 8ms/step - loss: 0.3504 - dense_9_loss: 0.2170 - dense_7_loss: 0.6668 - dense_9_acc: 0.9118 - dense_7_acc: 0.5627\n",
      "Epoch 10/50\n",
      "510/510 [==============================] - 4s 8ms/step - loss: 0.3454 - dense_9_loss: 0.2126 - dense_7_loss: 0.6643 - dense_9_acc: 0.9098 - dense_7_acc: 0.5627\n",
      "Epoch 11/50\n",
      "510/510 [==============================] - 4s 8ms/step - loss: 0.3468 - dense_9_loss: 0.2145 - dense_7_loss: 0.6613 - dense_9_acc: 0.9020 - dense_7_acc: 0.5627\n",
      "Epoch 12/50\n",
      "510/510 [==============================] - 4s 8ms/step - loss: 0.3280 - dense_9_loss: 0.1961 - dense_7_loss: 0.6596 - dense_9_acc: 0.9118 - dense_7_acc: 0.5627\n",
      "Epoch 13/50\n",
      "510/510 [==============================] - 4s 8ms/step - loss: 0.3299 - dense_9_loss: 0.1985 - dense_7_loss: 0.6566 - dense_9_acc: 0.9137 - dense_7_acc: 0.5627\n",
      "Epoch 14/50\n",
      "510/510 [==============================] - 4s 9ms/step - loss: 0.3225 - dense_9_loss: 0.1918 - dense_7_loss: 0.6534 - dense_9_acc: 0.9118 - dense_7_acc: 0.5627\n",
      "Epoch 15/50\n",
      "510/510 [==============================] - 5s 9ms/step - loss: 0.3240 - dense_9_loss: 0.1939 - dense_7_loss: 0.6504 - dense_9_acc: 0.9059 - dense_7_acc: 0.5627\n",
      "Epoch 16/50\n",
      "510/510 [==============================] - 4s 9ms/step - loss: 0.3239 - dense_9_loss: 0.1944 - dense_7_loss: 0.6471 - dense_9_acc: 0.9176 - dense_7_acc: 0.5627\n",
      "Epoch 17/50\n",
      "510/510 [==============================] - 4s 8ms/step - loss: 0.3120 - dense_9_loss: 0.1831 - dense_7_loss: 0.6445 - dense_9_acc: 0.9078 - dense_7_acc: 0.5627\n",
      "Epoch 18/50\n",
      "510/510 [==============================] - 4s 8ms/step - loss: 0.3042 - dense_9_loss: 0.1761 - dense_7_loss: 0.6405 - dense_9_acc: 0.9137 - dense_7_acc: 0.5627\n",
      "Epoch 19/50\n",
      "510/510 [==============================] - 4s 9ms/step - loss: 0.3075 - dense_9_loss: 0.1800 - dense_7_loss: 0.6371 - dense_9_acc: 0.9196 - dense_7_acc: 0.5706\n",
      "Epoch 20/50\n",
      "510/510 [==============================] - 4s 8ms/step - loss: 0.3153 - dense_9_loss: 0.1886 - dense_7_loss: 0.6332 - dense_9_acc: 0.9098 - dense_7_acc: 0.5725\n",
      "Epoch 21/50\n",
      "510/510 [==============================] - 4s 9ms/step - loss: 0.3055 - dense_9_loss: 0.1797 - dense_7_loss: 0.6285 - dense_9_acc: 0.9157 - dense_7_acc: 0.5765\n",
      "Epoch 22/50\n",
      "510/510 [==============================] - 4s 8ms/step - loss: 0.3026 - dense_9_loss: 0.1779 - dense_7_loss: 0.6232 - dense_9_acc: 0.9118 - dense_7_acc: 0.6118\n",
      "Epoch 23/50\n",
      "510/510 [==============================] - 4s 8ms/step - loss: 0.3021 - dense_9_loss: 0.1784 - dense_7_loss: 0.6188 - dense_9_acc: 0.9078 - dense_7_acc: 0.6961\n",
      "Epoch 24/50\n",
      "510/510 [==============================] - 4s 8ms/step - loss: 0.3071 - dense_9_loss: 0.1845 - dense_7_loss: 0.6130 - dense_9_acc: 0.9098 - dense_7_acc: 0.6667\n",
      "Epoch 25/50\n",
      "510/510 [==============================] - 5s 10ms/step - loss: 0.2936 - dense_9_loss: 0.1721 - dense_7_loss: 0.6076 - dense_9_acc: 0.9176 - dense_7_acc: 0.6216\n",
      "Epoch 26/50\n",
      "510/510 [==============================] - 4s 8ms/step - loss: 0.3027 - dense_9_loss: 0.1823 - dense_7_loss: 0.6021 - dense_9_acc: 0.9118 - dense_7_acc: 0.6863\n",
      "Epoch 27/50\n",
      "510/510 [==============================] - 4s 9ms/step - loss: 0.2914 - dense_9_loss: 0.1721 - dense_7_loss: 0.5963 - dense_9_acc: 0.9157 - dense_7_acc: 0.8137\n",
      "Epoch 28/50\n",
      "510/510 [==============================] - 5s 11ms/step - loss: 0.2881 - dense_9_loss: 0.1702 - dense_7_loss: 0.5899 - dense_9_acc: 0.9157 - dense_7_acc: 0.8137\n",
      "Epoch 29/50\n",
      "510/510 [==============================] - 5s 11ms/step - loss: 0.2815 - dense_9_loss: 0.1650 - dense_7_loss: 0.5825 - dense_9_acc: 0.9176 - dense_7_acc: 0.7902\n",
      "Epoch 30/50\n",
      "510/510 [==============================] - 5s 9ms/step - loss: 0.2811 - dense_9_loss: 0.1662 - dense_7_loss: 0.5744 - dense_9_acc: 0.9216 - dense_7_acc: 0.8118\n",
      "Epoch 31/50\n",
      "510/510 [==============================] - 5s 9ms/step - loss: 0.2713 - dense_9_loss: 0.1579 - dense_7_loss: 0.5667 - dense_9_acc: 0.9275 - dense_7_acc: 0.8627\n",
      "Epoch 32/50\n",
      "510/510 [==============================] - 4s 9ms/step - loss: 0.2709 - dense_9_loss: 0.1593 - dense_7_loss: 0.5577 - dense_9_acc: 0.9196 - dense_7_acc: 0.8569\n",
      "Epoch 33/50\n",
      "510/510 [==============================] - 6s 11ms/step - loss: 0.2724 - dense_9_loss: 0.1623 - dense_7_loss: 0.5503 - dense_9_acc: 0.9176 - dense_7_acc: 0.8392\n",
      "Epoch 34/50\n",
      "510/510 [==============================] - 6s 13ms/step - loss: 0.2627 - dense_9_loss: 0.1548 - dense_7_loss: 0.5392 - dense_9_acc: 0.9235 - dense_7_acc: 0.8824\n",
      "Epoch 35/50\n",
      "510/510 [==============================] - 6s 11ms/step - loss: 0.2610 - dense_9_loss: 0.1551 - dense_7_loss: 0.5297 - dense_9_acc: 0.9196 - dense_7_acc: 0.8804\n",
      "Epoch 36/50\n",
      "510/510 [==============================] - 5s 9ms/step - loss: 0.2592 - dense_9_loss: 0.1551 - dense_7_loss: 0.5207 - dense_9_acc: 0.9294 - dense_7_acc: 0.8667\n",
      "Epoch 37/50\n",
      "510/510 [==============================] - 5s 9ms/step - loss: 0.2618 - dense_9_loss: 0.1597 - dense_7_loss: 0.5106 - dense_9_acc: 0.9235 - dense_7_acc: 0.8784\n",
      "Epoch 38/50\n",
      "510/510 [==============================] - 5s 9ms/step - loss: 0.2606 - dense_9_loss: 0.1603 - dense_7_loss: 0.5015 - dense_9_acc: 0.9176 - dense_7_acc: 0.8824\n",
      "Epoch 39/50\n",
      "510/510 [==============================] - 4s 8ms/step - loss: 0.2516 - dense_9_loss: 0.1532 - dense_7_loss: 0.4919 - dense_9_acc: 0.9275 - dense_7_acc: 0.8824\n",
      "Epoch 40/50\n",
      "510/510 [==============================] - 4s 9ms/step - loss: 0.2546 - dense_9_loss: 0.1576 - dense_7_loss: 0.4847 - dense_9_acc: 0.9294 - dense_7_acc: 0.8882\n",
      "Epoch 41/50\n",
      "510/510 [==============================] - 4s 8ms/step - loss: 0.2550 - dense_9_loss: 0.1597 - dense_7_loss: 0.4766 - dense_9_acc: 0.9216 - dense_7_acc: 0.8686\n",
      "Epoch 42/50\n",
      "510/510 [==============================] - 4s 8ms/step - loss: 0.2452 - dense_9_loss: 0.1524 - dense_7_loss: 0.4637 - dense_9_acc: 0.9216 - dense_7_acc: 0.8843\n",
      "Epoch 43/50\n",
      "510/510 [==============================] - 4s 8ms/step - loss: 0.2491 - dense_9_loss: 0.1576 - dense_7_loss: 0.4574 - dense_9_acc: 0.9275 - dense_7_acc: 0.8824\n",
      "Epoch 44/50\n",
      "510/510 [==============================] - 4s 8ms/step - loss: 0.2390 - dense_9_loss: 0.1491 - dense_7_loss: 0.4499 - dense_9_acc: 0.9255 - dense_7_acc: 0.8804\n",
      "Epoch 45/50\n",
      "510/510 [==============================] - 5s 10ms/step - loss: 0.2398 - dense_9_loss: 0.1511 - dense_7_loss: 0.4433 - dense_9_acc: 0.9235 - dense_7_acc: 0.8804\n",
      "Epoch 46/50\n",
      "510/510 [==============================] - 5s 9ms/step - loss: 0.2423 - dense_9_loss: 0.1558 - dense_7_loss: 0.4324 - dense_9_acc: 0.9255 - dense_7_acc: 0.8863\n",
      "Epoch 47/50\n",
      "510/510 [==============================] - 5s 10ms/step - loss: 0.2324 - dense_9_loss: 0.1479 - dense_7_loss: 0.4228 - dense_9_acc: 0.9314 - dense_7_acc: 0.8902\n",
      "Epoch 48/50\n",
      "510/510 [==============================] - 4s 9ms/step - loss: 0.2354 - dense_9_loss: 0.1524 - dense_7_loss: 0.4151 - dense_9_acc: 0.9196 - dense_7_acc: 0.8902\n",
      "Epoch 49/50\n",
      "510/510 [==============================] - 4s 9ms/step - loss: 0.2233 - dense_9_loss: 0.1417 - dense_7_loss: 0.4081 - dense_9_acc: 0.9353 - dense_7_acc: 0.8941\n",
      "Epoch 50/50\n",
      "510/510 [==============================] - 4s 9ms/step - loss: 0.2322 - dense_9_loss: 0.1511 - dense_7_loss: 0.4054 - dense_9_acc: 0.9235 - dense_7_acc: 0.8882\n",
      "170/170 [==============================] - 1s 4ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10% (2 of 20) |##                       | Elapsed Time: 0:04:10 ETA:   0:37:36/home/elandg/eecs583-paper-end2end-dl/env/lib/python3.7/site-packages/ipykernel_launcher.py:4: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Correct?</th>\n",
       "      <th>Speedup</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Platform</th>\n",
       "      <th>Benchmark Suite</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"7\" valign=\"top\">AMD Tahiti 7970</th>\n",
       "      <th>AMD SDK</th>\n",
       "      <td>0.714286</td>\n",
       "      <td>1.037145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NPB</th>\n",
       "      <td>0.787879</td>\n",
       "      <td>3.095299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NVIDIA SDK</th>\n",
       "      <td>0.750000</td>\n",
       "      <td>3.736255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Parboil</th>\n",
       "      <td>0.833333</td>\n",
       "      <td>13.612841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Polybench</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.408824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rodinia</th>\n",
       "      <td>0.285714</td>\n",
       "      <td>5.661230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SHOC</th>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.979293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"7\" valign=\"top\">NVIDIA GTX 970</th>\n",
       "      <th>AMD SDK</th>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.742213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NPB</th>\n",
       "      <td>0.833333</td>\n",
       "      <td>1.462055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NVIDIA SDK</th>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.107079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Parboil</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.100187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Polybench</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.182752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rodinia</th>\n",
       "      <td>0.428571</td>\n",
       "      <td>1.059434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SHOC</th>\n",
       "      <td>0.777778</td>\n",
       "      <td>1.824558</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Correct?    Speedup\n",
       "Platform        Benchmark Suite                     \n",
       "AMD Tahiti 7970 AMD SDK          0.714286   1.037145\n",
       "                NPB              0.787879   3.095299\n",
       "                NVIDIA SDK       0.750000   3.736255\n",
       "                Parboil          0.833333  13.612841\n",
       "                Polybench        1.000000   2.408824\n",
       "                Rodinia          0.285714   5.661230\n",
       "                SHOC             0.888889   0.979293\n",
       "NVIDIA GTX 970  AMD SDK          0.571429   0.742213\n",
       "                NPB              0.833333   1.462055\n",
       "                NVIDIA SDK       0.750000   1.107079\n",
       "                Parboil          0.666667   1.100187\n",
       "                Polybench        1.000000   1.182752\n",
       "                Rodinia          0.428571   1.059434\n",
       "                SHOC             0.777778   1.824558"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "print(\"Evaluating DeepTune ...\", file=sys.stderr)\n",
    "deeptune = evaluate(deeptune_model)\n",
    "deeptune.groupby(['Platform', 'Benchmark Suite'])['Platform', 'Correct?', 'Speedup'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def predict(self, **test):\n",
    "#         p = np.array(self.model.predict(\n",
    "#             [test[\"aux_in\"], test[\"sequences\"]], batch_size=64, verbose=test[\"verbose\"]))\n",
    "#         indices = [np.argmax(x) for x in p[0]]\n",
    "#         return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 32ms/step\n",
      "Predicted: CPU\n"
     ]
    }
   ],
   "source": [
    "srcs = [rewritten]\n",
    "srcs = encode_srcs(srcs)\n",
    "\n",
    "df = pd.read_csv(f\"../data/case-study-a/cgo17-amd.csv\")\n",
    "df_row = df.iloc[[630]]\n",
    "\n",
    "features = grewe_features(df_row)\n",
    "aux_in = auxiliary_inputs(df_row)\n",
    "\n",
    "p = deeptune_model.predict_demo(aux_in=aux_in, sequences=srcs, verbose=True)\n",
    "mapping = {\n",
    "    0: \"CPU\",\n",
    "    1: \"GPU\"\n",
    "}\n",
    "print(\"Predicted:\", mapping[p[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Unnamed: 0                                benchmark  dataset  comp  \\\n",
      "630         630  rodinia-3.1-streamcluster-memset_kernel  default     0   \n",
      "\n",
      "     rational  mem  localmem  coalesced  atomic  transfer  wgsize oracle  \\\n",
      "630         0    1         0          1       0  80543744     256    CPU   \n",
      "\n",
      "     runtime_cpu  runtime_gpu  \\\n",
      "630   777.894567  1141.490884   \n",
      "\n",
      "                                                   src  \\\n",
      "630  __kernel void A(__global char* a, short b, int...   \n",
      "\n",
      "                                seq  \n",
      "630  [129 129 129 ...,  26   0 127]  \n"
     ]
    }
   ],
   "source": [
    "print(df_row)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "cbac100c1e935252d359d825fd3ac71dfe3b9a8ec214ef04a342f5b5ae0ac34d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
