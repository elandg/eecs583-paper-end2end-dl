{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenCL Heterogeneous Mapping\n",
    "\n",
    "This notebook contains the experiments for Case Study A - using deep learning to predict the optimal device mapping for OpenCL kernels, without hand engineered features.\n",
    "\n",
    "## 1. Runtime Data\n",
    "\n",
    "We use the runtime data from our CGO'17 paper [1]. The code to reproduce those experiments can be found in this, artifact evaluated [GitHub repository](https://github.com/ChrisCummins/paper-synthesizing-benchmarks).\n",
    "\n",
    "> [1] Cummins, C., Petoumenos, P., Zang, W., & Leather, H. (2017). [Synthesizing Benchmarks for Predictive Modeling](http://chriscummins.cc/pub/2017-cgo.pdf). In CGO. IEEE.\n",
    "\n",
    "The runtime data comes from two plaforms. AMD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 5)\n",
    "\n",
    "pd.read_csv(\"../data/case-study-a/cgo17-amd.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and NVIDIA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(\"../data/case-study-a/cgo17-nvidia.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Predictive Models\n",
    "\n",
    "We define a base class for implementing predictive models for heterogeneous mapping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from clgen import _atomizer as clgen\n",
    "\n",
    "class HeterogemeousMappingModel(object):\n",
    "    \"\"\"\n",
    "    A model for predicting OpenCL heterogeneous device mappings.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    __name__ : str\n",
    "        Model name\n",
    "    __basename__ : str\n",
    "        Shortened name, used for files\n",
    "    \"\"\"\n",
    "    __name__ = None\n",
    "    __basename__ = None\n",
    "    \n",
    "    def init(self, seed: int) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the model.\n",
    "        \n",
    "        Do whatever is required to setup a new heterogeneous model here.\n",
    "        This method is called prior to training and predicting.\n",
    "        This method may be omitted if no initial setup is required.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        seed : int\n",
    "            The seed value used to reproducible results. May be 'None',\n",
    "            indicating that no seed is to be used.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def save(self, outpath: str) -> None:\n",
    "        \"\"\"\n",
    "        Save model state.\n",
    "        \n",
    "        This must capture all of the relevant state of the model. It is up\n",
    "        to implementing classes to determine how best to save the model.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        outpath : str\n",
    "            The path to save the model state to.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def restore(self, inpath: str) -> None:\n",
    "        \"\"\"\n",
    "        Load a trained model from file.\n",
    "        \n",
    "        This is called in place of init() if a saved model file exists. It\n",
    "        must restore all of the required model state.\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        inpath : str\n",
    "            The path to load the model from. This is the same path as\n",
    "            was passed to save() to create the file.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def train(self, df: pd.DataFrame, features: np.array, sequences: np.array,\n",
    "              y: np.array, y_1hot: np.array, verbose: bool=False) -> None:\n",
    "        \"\"\"\n",
    "        Train a model.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        df : pd.DataFrame\n",
    "            The platform dataframe.\n",
    "        \n",
    "        features : np.array\n",
    "            An array of feature vectors of shape (n,4).\n",
    "        \n",
    "        sequences : np.array\n",
    "            An array of encoded source code sequences of shape (n,seq_length).\n",
    "\n",
    "        y : np.array\n",
    "            An array of optimal device mappings of shape (n,1).\n",
    "        \n",
    "        y_1hot : np.array\n",
    "            An array of optimal device mappings of shape (n,2), in 1-hot encoding.\n",
    "            \n",
    "        verbose: bool, optional\n",
    "            Whether to print verbose status messages during training.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def predict(self, features: np.array, sequences: np.array, y: np.array,\n",
    "                y_1hot: np.array, verbose: bool=False) -> np.array:\n",
    "        \"\"\"\n",
    "        Make predictions for programs.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        features : np.array\n",
    "            An array of feature vectors of shape (n,4).\n",
    "        \n",
    "        sequences : np.array\n",
    "            An array of encoded source code sequences of shape (n,seq_length).\n",
    "\n",
    "        y : np.array\n",
    "            An array of optimal device mappings of shape (n,1).\n",
    "        \n",
    "        y_1hot : np.array\n",
    "            An array of optimal device mappings of shape (n,2), in 1-hot encoding.\n",
    "            \n",
    "        verbose: bool, optional\n",
    "            Whether to print verbose status messages.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        np.array\n",
    "            Predicted 'y' values (optimal device mappings) with shape (n,1).\n",
    "        \"\"\"\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also define some utility code which we will use later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def platform2str(platform: str) -> str:\n",
    "    \"\"\" get full platform name \"\"\"\n",
    "    if platform == \"amd\":\n",
    "        return \"AMD Tahiti 7970\"\n",
    "    elif platform == \"nvidia\":\n",
    "        return \"NVIDIA GTX 970\"\n",
    "    else:\n",
    "        raise LookupException\n",
    "        \n",
    "def escape_suite_name(g: str) -> str:\n",
    "    \"\"\" format benchmark suite name for display \"\"\"\n",
    "    c = g.split('-')\n",
    "    if c[0] == \"amd\" or c[0] == \"nvidia\":\n",
    "        return c[0].upper() + \" SDK\"\n",
    "    if c[0] == \"npb\" or c[0] == \"shoc\":\n",
    "        return c[0].upper()\n",
    "    elif c[0] == \"parboil\" or c[0] == \"polybench\" or c[0] == \"rodinia\":\n",
    "        return c[0].capitalize()\n",
    "    else:\n",
    "        raise LookupError\n",
    "\n",
    "\n",
    "def escape_benchmark_name(g: str) -> str:\n",
    "    \"\"\"escape benchmark name for display\"\"\"\n",
    "    c = g.split('-')\n",
    "    return escape_suite_name(c[0]).split()[0] + \".\" + c[-2]\n",
    "\n",
    "    \n",
    "def grewe_features(df: pd.DataFrame) -> np.array:\n",
    "    \"\"\" extract Grewe et al. feature vector from runtime data \"\"\"\n",
    "    return np.array([\n",
    "            (df[\"transfer\"].values / (df[\"comp\"].values + df[\"mem\"].values)),  # F1\n",
    "            (df[\"coalesced\"].values / df[\"mem\"].values),  # F2\n",
    "            ((df[\"localmem\"].values / df[\"mem\"].values) * df[\"wgsize\"].values),  # F3\n",
    "            (df[\"comp\"].values / df[\"mem\"].values),  # F4\n",
    "        ]).T\n",
    "\n",
    "def auxiliary_inputs(df: pd.DataFrame) -> np.array:\n",
    "    \"\"\" get dsize and wgsize auxiliary inputs \"\"\"\n",
    "    return np.array([\n",
    "        df[\"transfer\"].values,\n",
    "        df[\"wgsize\"].values,\n",
    "    ]).T\n",
    "\n",
    "def encode_1hot(y: np.array) -> np.array:\n",
    "    \"\"\" 1-hot encode labels \"\"\"\n",
    "    labels = np.vstack([np.expand_dims(x, axis=0) for x in y])\n",
    "    l2 = [x[0] for x in labels]\n",
    "    l1 = [not x for x in l2]\n",
    "    return np.array(list(zip(l1, l2)), dtype=np.int32)\n",
    "\n",
    "def encode_srcs(srcs: List[str]) -> np.array:\n",
    "    \"\"\" encode and pad source code for learning \"\"\"\n",
    "    from keras.preprocessing.sequence import pad_sequences\n",
    "    \n",
    "    seqs = [atomizer.atomize(src) for src in srcs]\n",
    "    pad_val = atomizer.vocab_size\n",
    "    encoded = np.array(pad_sequences(seqs, maxlen=1024, value=pad_val))\n",
    "    return np.vstack([np.expand_dims(x, axis=0) for x in encoded])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Experimental Methodology\n",
    "\n",
    "Random seed for reproducible results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seed = 204"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The source encoder (see the *'Language Model.ipynb'* notebook for details):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clgen import Language\n",
    "\n",
    "srcs = '\\n'.join(pd.read_csv(\"../data/case-study-a/cgo17-amd.csv\")['src'].values)\n",
    "atomizer = clgen.GreedyAtomizer.from_text(lang=Language.from_str(\"opencl\"), text=srcs)\n",
    "atomizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our method to evaluate a predictive model using 10-fold cross-validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from labm8 import fs\n",
    "\n",
    "def evaluate(model: HeterogemeousMappingModel) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Evaluate a model.\n",
    "    \n",
    "    Performs 10-fold cross-validation of the model's effectiveness at predicting\n",
    "    OpenCL device mappings. Results are cached.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : HeterogeneousMappingModel\n",
    "        The predictive model to evaluate.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pd.Dataframe\n",
    "        Evaluation results.\n",
    "    \"\"\"\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "    from progressbar import ProgressBar\n",
    "    \n",
    "    progressbar = [0, ProgressBar(max_value=10*2)]\n",
    "\n",
    "    data = []\n",
    "    for i, platform in enumerate([\"amd\", \"nvidia\"]):\n",
    "        platform_name = platform2str(platform)\n",
    "\n",
    "        # load runtime data\n",
    "        df = pd.read_csv(f\"../data/case-study-a/cgo17-{platform}.csv\")\n",
    "        \n",
    "        sequences = None  # defer sequence encoding until needed (it's expensive)\n",
    "        \n",
    "        # values used for training & predictions\n",
    "        features = grewe_features(df)\n",
    "        aux_in = auxiliary_inputs(df)\n",
    "        \n",
    "        # optimal mappings\n",
    "        y = np.array([1 if x == \"GPU\" else 0 for x in df[\"oracle\"].values])\n",
    "        y_1hot = encode_1hot(y)\n",
    "        \n",
    "        # 10-fold cross-validation\n",
    "        kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "        for j, (train_index, test_index) in enumerate(kf.split(features, y)):\n",
    "            model_path = f\"../data/case-study-a/models/{model.__basename__}-{platform}-{j}.model\"\n",
    "            predictions_path = f\"../data/case-study-a/predictions/{model.__basename__}-{platform}-{j}.result\"\n",
    "            \n",
    "            if fs.exists(predictions_path):\n",
    "                # load result from cache\n",
    "                with open(predictions_path, 'rb') as infile:\n",
    "                    p = pickle.load(infile)\n",
    "            else:\n",
    "                if sequences is None:  # encode source codes if needed\n",
    "                    sequences = encode_srcs(df[\"src\"].values)\n",
    "\n",
    "                if fs.exists(model_path):\n",
    "                    # restore trained model from cache\n",
    "                    model.restore(model_path)\n",
    "                else:\n",
    "                    # train and cache a model\n",
    "                    model.init(seed=seed)                   \n",
    "                    model.train(df=df,\n",
    "                                features=features[train_index],\n",
    "                                aux_in=aux_in[train_index],\n",
    "                                sequences=sequences[train_index],\n",
    "                                y=y[train_index],\n",
    "                                y_1hot=y_1hot[train_index],\n",
    "                                verbose=False)\n",
    "                    fs.mkdir(fs.dirname(model_path))\n",
    "                    model.save(model_path)\n",
    "\n",
    "                # test model\n",
    "                p = model.predict(\n",
    "                    features=features[test_index],\n",
    "                    aux_in=aux_in[test_index],\n",
    "                    sequences=sequences[test_index],\n",
    "                    y=y[test_index],\n",
    "                    y_1hot=y_1hot[test_index],\n",
    "                    verbose=False)\n",
    "    \n",
    "                # cache results\n",
    "                fs.mkdir(fs.dirname(predictions_path))\n",
    "                with open(predictions_path, 'wb') as outfile:\n",
    "                    pickle.dump(p, outfile)\n",
    "\n",
    "            # benchmarks\n",
    "            benchmarks = df['benchmark'].values[test_index]\n",
    "            # oracle device mappings\n",
    "            o = y[test_index]\n",
    "            # whether predictions were correct or not\n",
    "            correct = p == o\n",
    "            # runtimes of baseline mapping (CPU on AMD, GPU on NVIDIA)\n",
    "            zero_r_dev = \"runtime_cpu\" if platform == \"amd\" else \"runtime_gpu\"\n",
    "            zer_r_runtimes = df[zero_r_dev][test_index]\n",
    "            # speedups of predictions\n",
    "            runtimes = df[['runtime_cpu', 'runtime_gpu']].values[test_index]\n",
    "            p_runtimes = [r[p_] for p_, r in zip(p, runtimes)]\n",
    "            p_speedup = zer_r_runtimes / p_runtimes\n",
    "    \n",
    "            # sanity check\n",
    "            assert(len(benchmarks) == len(o) == len(correct) == len(p) == len(p_speedup))\n",
    "\n",
    "            # record results\n",
    "            for benchmark_, o_, p_, correct_, p_speedup_ in zip(benchmarks, o, p, correct, p_speedup):\n",
    "                data.append({\n",
    "                    \"Model\": model.__name__,\n",
    "                    \"Platform\": platform_name,\n",
    "                    'Benchmark': escape_benchmark_name(benchmark_),\n",
    "                    'Benchmark Suite': escape_suite_name(benchmark_),\n",
    "                    \"Oracle Mapping\": o_,\n",
    "                    \"Predicted Mapping\": p_,\n",
    "                    \"Correct?\": correct_,\n",
    "                    \"Speedup\": p_speedup_,\n",
    "                })\n",
    "            \n",
    "            # update progress bar\n",
    "            progressbar[0] += 1\n",
    "            progressbar[1].update(progressbar[0])\n",
    "\n",
    "    return pd.DataFrame(\n",
    "        data, index=range(1, len(data)+1), columns=[\n",
    "            \"Model\",\n",
    "            \"Platform\",\n",
    "            \"Benchmark\",\n",
    "            \"Benchmark Suite\", \n",
    "            \"Oracle Mapping\", \n",
    "            \"Predicted Mapping\", \n",
    "            \"Correct?\", \n",
    "            \"Speedup\"\n",
    "        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Static Mapping\n",
    "\n",
    "A static mapping selects the device which is most frequently optimal for that platform, i.e. it is the upper bound of performance which can be achieved without a heuristic. We use this as a baseline to compare predictive models against:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "class StaticMapping(HeterogemeousMappingModel):\n",
    "    __name__ = \"Static mapping\"\n",
    "    __basename__ = \"static\"\n",
    "    \n",
    "    def init(self, seed: int): return self\n",
    "    \n",
    "    def save(self, outpath):\n",
    "        with open(outpath, \"wb\") as outfile:\n",
    "            pickle.dump(self.model, outfile)\n",
    "\n",
    "    def restore(self, inpath):\n",
    "        with open(inpath, \"rb\") as infile:\n",
    "            self.model = pickle.load(infile)\n",
    "\n",
    "    def train(self, df=None, **train):\n",
    "        from collections import Counter\n",
    "        \n",
    "        # select the Zero-R device: the most frequently optimal device\n",
    "        zero_r_device = Counter(df['oracle']).most_common(1)[0][0]\n",
    "        self.model = 1 if zero_r_device == \"GPU\" else 0\n",
    "\n",
    "    def predict(self, **test):\n",
    "        if self.model:\n",
    "            return np.ones(len(test[\"y\"])).astype(np.int32)\n",
    "        else:\n",
    "            return np.zeros(len(test[\"y\"])).astype(dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "pd.set_option('display.max_rows', 14)\n",
    "\n",
    "print(\"Evaluating static mapping ...\", file=sys.stderr)\n",
    "baseline = evaluate(StaticMapping())\n",
    "baseline.groupby(['Platform', 'Benchmark Suite'])['Platform', 'Correct?', 'Speedup'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary of results. The *Correct?* column shows model accuracy. In the paper we report that the static mapping is accurate for **58.8%** of cases on AMD and **56.9%** of cases on NVIDIA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline.groupby(['Platform'])['Platform', 'Correct?', 'Speedup'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Grewe et al. Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Grewe et al. predictive model uses decision trees and hand engineered features to predict optimal device mapping, described in publication:\n",
    "\n",
    "> [2] Grewe, D., Wang, Z., & O’Boyle, M. (2013). [Portable Mapping of Data Parallel Programs to OpenCL for Heterogeneous Systems](http://www.ece.neu.edu/groups/nucar/NUCARTALKS/cgo2013-grewe.pdf). In CGO. IEEE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Grewe(HeterogemeousMappingModel):\n",
    "    __name__ = \"Grewe et al.\"\n",
    "    __basename__ = \"grewe\"\n",
    "    \n",
    "    def init(self, seed: int):\n",
    "        from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "        self.model = DecisionTreeClassifier(\n",
    "            random_state=seed, splitter=\"best\",\n",
    "            criterion=\"entropy\", max_depth=5,\n",
    "            min_samples_leaf=5)\n",
    "        return self\n",
    "    \n",
    "    def save(self, outpath):\n",
    "        with open(outpath, \"wb\") as outfile:\n",
    "            pickle.dump(self.model, outfile)\n",
    "\n",
    "    def restore(self, inpath):\n",
    "        with open(inpath, \"rb\") as infile:\n",
    "            self.model = pickle.load(infile)\n",
    "        \n",
    "    def train(self, **train):\n",
    "        self.model.fit(train[\"features\"], train[\"y\"])\n",
    "\n",
    "    def predict(self, **test):\n",
    "        return self.model.predict(test[\"features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluating Grewe et al. ...\", file=sys.stderr)\n",
    "grewe = evaluate(Grewe())\n",
    "grewe.groupby(['Platform', 'Benchmark Suite'])['Platform', 'Correct?', 'Speedup'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary of results. The *Correct?* column shows model accuracy. In the paper we report that the Grewe et al. model is accurate **73%** of cases, achieving speedups of **2.91$\\times$** on AMD and **1.26$\\times$** on NVIDIA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grewe.groupby(['Platform'])['Platform', 'Correct?', 'Speedup'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. DeepTune\n",
    "\n",
    "We predict optimal device mapping from raw source code inputs, and the\n",
    "two auxiliary inputs wgsize and dsize (which cannot be obtained statically\n",
    "from the program source):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DeepTune(HeterogemeousMappingModel):\n",
    "    __name__ = \"DeepTune\"\n",
    "    __basename__ = \"deeptune\"\n",
    "    \n",
    "    def init(self, seed: int):\n",
    "        from keras.layers import Input, Embedding, LSTM, Dense\n",
    "        from keras.layers.merge import Concatenate\n",
    "        from keras.layers.normalization import BatchNormalization\n",
    "        from keras.models import Model\n",
    "\n",
    "        np.random.seed(seed)\n",
    "        \n",
    "        # Language model. Takes as inputs source code sequences.\n",
    "        code_in = Input(shape=(1024,), dtype=\"int32\", name=\"code_in\")\n",
    "        x = Embedding(input_dim=atomizer.vocab_size + 1, input_length=1024, \n",
    "                      output_dim=64, name=\"embedding\")(code_in)\n",
    "        x = LSTM(64, implementation=1, return_sequences=True, name=\"lstm_1\")(x)\n",
    "        x = LSTM(64, implementation=1, name=\"lstm_2\")(x)\n",
    "        langmodel_out = Dense(2, activation=\"sigmoid\")(x)\n",
    "        \n",
    "        # Auxiliary inputs. wgsize and dsize.\n",
    "        auxiliary_inputs = Input(shape=(2,))\n",
    "        \n",
    "        # Heuristic model. Takes as inputs the language model,\n",
    "        #   outputs 1-hot encoded device mapping\n",
    "        x = Concatenate()([auxiliary_inputs, x])\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dense(32, activation=\"relu\")(x)\n",
    "        out = Dense(2, activation=\"sigmoid\")(x)\n",
    "\n",
    "        self.model = Model(inputs=[auxiliary_inputs, code_in], outputs=[out, langmodel_out])\n",
    "        self.model.compile(\n",
    "            optimizer=\"adam\", metrics=['accuracy'],\n",
    "            loss=[\"categorical_crossentropy\", \"categorical_crossentropy\"],\n",
    "            loss_weights=[1., .2])\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def save(self, outpath):\n",
    "        self.model.save(outpath)\n",
    "\n",
    "    def restore(self, inpath):\n",
    "        from keras.models import load_model\n",
    "        self.model = load_model(inpath)\n",
    "        \n",
    "    def train(self, **train):\n",
    "        self.model.fit([train[\"aux_in\"], train[\"sequences\"]], [train[\"y_1hot\"], train[\"y_1hot\"]],\n",
    "                       epochs=50, batch_size=64, verbose=train[\"verbose\"], shuffle=True)\n",
    "\n",
    "    def predict(self, **test):\n",
    "        p = np.array(self.model.predict(\n",
    "            [test[\"aux_in\"], test[\"sequences\"]], batch_size=64, verbose=test[\"verbose\"]))\n",
    "        indices = [np.argmax(x) for x in p[0]]\n",
    "        return indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An overview of the DeepTune model, showing the number of parameters in each layer, used to construct Table 5 in the paper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deeptune_model = DeepTune()\n",
    "deeptune_model.init(seed)\n",
    "deeptune_model.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isnotebook():\n",
    "    # written by @mtd http://stackoverflow.com/a/39662359\n",
    "    try:\n",
    "        shell = get_ipython().__class__.__name__\n",
    "        if shell == 'ZMQInteractiveShell':  # Jupyter notebook or qtconsole?\n",
    "            return True\n",
    "        elif shell == 'TerminalInteractiveShell':  # Terminal running IPython?\n",
    "            return False\n",
    "        else:\n",
    "            return False  # Other type (?)\n",
    "    except NameError:\n",
    "        return False      # Probably standard Python interpreter\n",
    "\n",
    "\n",
    "img = None\n",
    "if isnotebook():\n",
    "    from keras.utils.vis_utils import model_to_dot\n",
    "    from IPython.display import SVG\n",
    "    img = SVG(model_to_dot(deeptune_model.model, show_shapes=True).create(prog='dot', format='svg'))\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluating DeepTune ...\", file=sys.stderr)\n",
    "deeptune = evaluate(deeptune_model)\n",
    "deeptune.groupby(['Platform', 'Benchmark Suite'])['Platform', 'Correct?', 'Speedup'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary of results. The *Correct?* column shows model accuracy. In the paper we report that DeepTune is accurate for **82%** of cases, achieving speedups of **3.34$\\times$** on AMD and **1.41$\\times$** on NVIDIA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deeptune.groupby(['Platform'])['Platform', 'Correct?', 'Speedup'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluation\n",
    "\n",
    "Here we evaluate the quality of the models using two metrics: prediction accuracy, and performance relative to the static mapping:\n",
    "\n",
    "### 4.1. Prediction Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = []\n",
    "d.append(np.append(baseline.groupby(['Platform'])['Correct?'].mean().values * 100,\n",
    "                   baseline['Correct?'].mean() * 100))\n",
    "d.append(np.append(grewe.groupby(['Platform'])['Correct?'].mean().values * 100,\n",
    "                   grewe['Correct?'].mean() * 100))\n",
    "d.append(np.append(deeptune.groupby(['Platform'])['Correct?'].mean().values * 100,\n",
    "                   deeptune['Correct?'].mean() * 100))\n",
    "d = np.array(d).T.reshape(3, 3)\n",
    "\n",
    "pd.DataFrame(d, columns=['Static mapping', 'Grewe et al.', 'DeepTune'],\n",
    "             index=['AMD Tahiti 7970', 'NVIDIA GTX 970', 'Average'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 6 of the paper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if isnotebook():\n",
    "    import matplotlib\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "\n",
    "    from matplotlib.ticker import FormatStrFormatter\n",
    "    from labm8 import viz\n",
    "\n",
    "    # Plotting configuration\n",
    "    %matplotlib inline\n",
    "    sns.set(style=\"ticks\", color_codes=True)\n",
    "    plt.style.use([\"seaborn-white\", \"seaborn-paper\"])\n",
    "    matplotlib.rcParams['figure.dpi'] = 120\n",
    "    \n",
    "    for i, platform in enumerate([\"AMD Tahiti 7970\", \"NVIDIA GTX 970\"]):\n",
    "        \n",
    "        def get_accuracies(df, platform):\n",
    "            \"\"\" get % accuracies for platform, aggregated by benchmark suite \"\"\"\n",
    "            # average by benchmark suites\n",
    "            series = df[df[\"Platform\"] == platform]\\\n",
    "                        .groupby(['Benchmark Suite'])['Correct?'].mean() * 100  # %\n",
    "            # append average across all benchamrks\n",
    "            average = pd.Series(df[df[\"Platform\"] == platform]['Correct?'].mean() * 100,\n",
    "                                index=[\"Average\"])\n",
    "            return series.append(average)\n",
    "\n",
    "        # Aggregate data\n",
    "        models, results = (StaticMapping, Grewe, DeepTune), (baseline, grewe, deeptune)\n",
    "        dfs = [get_accuracies(r, platform) for r in results]\n",
    "\n",
    "        accuracies = np.concatenate([dfs[i].values for i in range(len(dfs))])\n",
    "        benchmarks = np.concatenate([dfs[i].keys().values for i in range(len(dfs))])\n",
    "        models = np.concatenate([np.array([m().__name__] * len(dfs[0])) for m in models])\n",
    "\n",
    "        benchmarks = np.append(benchmarks, \"Average\")\n",
    "\n",
    "        df = pd.DataFrame([{\"Benchmark\": b, \"Accuracy\": a, \"Model\": m}\n",
    "                           for b, a, m in zip(benchmarks, accuracies, models)])\n",
    "\n",
    "        # Plot\n",
    "        plt.subplot(2, 1, i + 1)\n",
    "        palette = sns.cubehelix_palette(3, rot=.2, light=.85, dark=.35)\n",
    "        ax = sns.barplot(x=\"Benchmark\", y=\"Accuracy\", hue=\"Model\", palette=palette, data=df)\n",
    "\n",
    "        # x axis\n",
    "        c = chr(ord('a') + i)\n",
    "        plt.xlabel(f\"({c}) {platform}\", fontsize=10)\n",
    "        plt.setp(ax.get_xticklabels(), rotation=25)  # rotate x ticks\n",
    "        plt.axvline(x=plt.xlim()[1] - 1, color=\"k\", lw=1, linestyle=\"--\")  # average line\n",
    "\n",
    "        # y axis\n",
    "        plt.ylim((0, 100))    \n",
    "        plt.gca().yaxis.set_major_formatter(FormatStrFormatter(\"%d%%\"))\n",
    "        plt.ylabel(\"Accuracy\")\n",
    "\n",
    "        # legend\n",
    "        if i == 0:\n",
    "            plt.legend(loc=(0.003, 1.05), ncol=3)\n",
    "            ax.get_legend().set_title(\"\")  # no legend title\n",
    "            ax.get_legend().draw_frame(True)\n",
    "        elif i == 1:\n",
    "            ax.legend().set_visible(False)\n",
    "\n",
    "    viz.finalise(\n",
    "        # TODO: fs.path(\"~/Inbox/cgo-acc.pdf\"),\n",
    "        figsize=(4.25, 4.8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Speedups\n",
    "\n",
    "Mean speedup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = []\n",
    "d.append(np.append(baseline.groupby(['Platform'])['Speedup'].mean().values,\n",
    "                   baseline['Speedup'].mean()))\n",
    "d.append(np.append(grewe.groupby(['Platform'])['Speedup'].mean().values,\n",
    "                   grewe['Speedup'].mean()))\n",
    "d.append(np.append(deeptune.groupby(['Platform'])['Speedup'].mean().values,\n",
    "                   deeptune['Speedup'].mean()))\n",
    "d = np.array(d).T.reshape(3, 3)\n",
    "\n",
    "pd.DataFrame(d, columns=['Static mapping', 'Grewe et al.', 'DeepTune'],\n",
    "             index=['AMD Tahiti 7970', 'NVIDIA GTX 970', 'Average'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the paper we report geometric means of **1.18$\\times$** for Grewe et al., and **1.31$\\times$** for DeepTune:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from labm8 import math as labmath\n",
    "\n",
    "grewe_geomean = labmath.geomean(grewe['Speedup'].values)\n",
    "deeptune_geomean = labmath.geomean(deeptune['Speedup'].values)\n",
    "\n",
    "print(f\"Geometric mean of Grewe et al. {grewe_geomean:.2f}x\")\n",
    "print(f\"Geometric mean of DeepTune {deeptune_geomean:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 7 of the paper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if isnotebook():\n",
    "    for i, platform in enumerate([\"AMD Tahiti 7970\", \"NVIDIA GTX 970\"]):\n",
    "\n",
    "        def get_speedups(df, platform):\n",
    "            \"\"\" get speedups for platform, aggregated by benchmark \"\"\"\n",
    "            # average by benchmark\n",
    "            series = df[df[\"Platform\"] == platform]\\\n",
    "                .groupby(['Benchmark'])['Speedup'].mean()\n",
    "            # average across all benchmarks\n",
    "            average = pd.Series(df[df['Platform'] == platform]['Speedup'].mean(), index=['Average'])\n",
    "            return series.append(average)\n",
    "\n",
    "        # Aggregate data\n",
    "        models, results = (Grewe, DeepTune), (grewe, deeptune)\n",
    "        dfs = [get_speedups(r, platform) for r in results]\n",
    "\n",
    "        speedups    = np.concatenate([dfs[i].values for i in range(len(dfs))])\n",
    "        benchmarks  = np.concatenate([dfs[i].keys().values for i in range(len(dfs))])\n",
    "        model_names = np.concatenate([np.array([m().__name__] * len(dfs[0])) for m in models])\n",
    "\n",
    "        df = pd.DataFrame([{\"Benchmark\": b, \"Speedup\": s - 1, \"Model\": m}\n",
    "                           for b, s, m in zip(benchmarks, speedups, model_names)])\n",
    "\n",
    "        # Plot\n",
    "        plt.subplot(2, 1, i + 1)\n",
    "        palette = sns.cubehelix_palette(2, rot=1.8, light=.85, dark=.35)\n",
    "        ax = sns.barplot(x=\"Benchmark\", y=\"Speedup\", hue=\"Model\", palette=palette, data=df)\n",
    "        c = chr(ord('a') + i)\n",
    "        plt.xlabel(f\"({c}) {platform}\", fontsize=10)\n",
    "        \n",
    "        # x axis\n",
    "        plt.setp(ax.get_xticklabels(), rotation=90)\n",
    "        plt.axvline(x=plt.xlim()[1] - 1, color=\"k\", lw=1, linestyle=\"--\")  # average line\n",
    "\n",
    "        # y axis\n",
    "        plt.ylim((-1, 3) if i else (-1, 9))\n",
    "        plt.ylabel(\"Speedup\", fontsize=10)\n",
    "\n",
    "        # legend\n",
    "        plt.legend(loc='upper left')\n",
    "        ax.get_legend().set_title(\"\")\n",
    "        ax.get_legend().draw_frame(True)\n",
    "\n",
    "        plt.axhline(y=0, color=\"k\", lw=.5)  # speedup line\n",
    "        ax.set_yticklabels([\"{:.1f}x\".format(i + 1) for i in ax.get_yticks()])\n",
    "    \n",
    "    viz.finalise(figsize=(8.5, 6.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Comparison to State-of-the-art\n",
    "\n",
    "Here we directly compare performance of DeepTune against the Grewe et al. state-of-the-art model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_improvement = (deeptune[\"Speedup\"].mean() / grewe[\"Speedup\"].mean()) - 1\n",
    "\n",
    "print(f\"DeepTune outperforms Grewe et al. by {performance_improvement:.0%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cases = len(deeptune)\n",
    "num_better = sum(1 for d, g in zip(deeptune[\"Speedup\"], grewe[\"Speedup\"]) if d >= g)\n",
    "ratio_better = num_better / num_cases\n",
    "print(\"DeepTune matched or improved over state-of-the-art in \"\n",
    "      f\"{num_better} of {num_cases} cases ({ratio_better:.1%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4. Training without Auxiliary Inputs\n",
    "\n",
    "Finally, we evaluate the role of auxiliary inputs on the quality of the learned model. We configure a DeepTune model without the two auxiliary inputs for workgroup and dataset size, and repeat the evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DeepTuneNoAux(HeterogemeousMappingModel):\n",
    "    __name__ = \"DeepTune wo. Auxiliary Inputs\"\n",
    "    __basename__ = \"deeptune-no-aux\"\n",
    "    \n",
    "    def init(self, seed: int):\n",
    "        from keras.layers import Input, Embedding, LSTM, Dense\n",
    "        from keras.layers.merge import Concatenate\n",
    "        from keras.layers.normalization import BatchNormalization\n",
    "        from keras.models import Model\n",
    "\n",
    "        np.random.seed(seed)\n",
    "        \n",
    "        # Language model. Takes as inputs source code sequences.\n",
    "        code_in = Input(shape=(1024,), dtype=\"int32\", name=\"code_in\")\n",
    "        x = Embedding(input_dim=atomizer.vocab_size + 1, input_length=1024, \n",
    "                      output_dim=64, name=\"embedding\")(code_in)\n",
    "        x = LSTM(64, implementation=1, return_sequences=True, name=\"lstm_1\")(x)\n",
    "        x = LSTM(64, implementation=1, name=\"lstm_2\")(x)\n",
    "            \n",
    "        # Heuristic model. Takes as inputs the language model,\n",
    "        #   outputs 1-hot encoded device mapping\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dense(32, activation=\"relu\")(x)\n",
    "        out = Dense(2, activation=\"sigmoid\")(x)\n",
    "\n",
    "        self.model = Model(inputs=[code_in], outputs=[out])\n",
    "        self.model.compile(\n",
    "            optimizer=\"adam\", metrics=['accuracy'],\n",
    "            loss=\"categorical_crossentropy\")\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def save(self, outpath):\n",
    "        self.model.save(outpath)\n",
    "\n",
    "    def restore(self, inpath):\n",
    "        from keras.models import load_model\n",
    "        self.model = load_model(inpath)\n",
    "        \n",
    "    def train(self, **train):\n",
    "        self.model.fit(train[\"sequences\"], train[\"y_1hot\"],\n",
    "                       epochs=50, batch_size=64, verbose=train[\"verbose\"], shuffle=True)\n",
    "\n",
    "    def predict(self, **test):\n",
    "        p = np.array(self.model.predict(test[\"sequences\"], batch_size=64, verbose=test[\"verbose\"]))\n",
    "        indices = [np.argmax(x) for x in p]\n",
    "        return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deeptune_noaux_model = DeepTuneNoAux()\n",
    "deeptune_noaux_model.init(seed)\n",
    "deeptune_noaux_model.model.summary()\n",
    "\n",
    "img = None\n",
    "if isnotebook():\n",
    "    from keras.utils.vis_utils import model_to_dot\n",
    "    from IPython.display import SVG\n",
    "    img = SVG(model_to_dot(deeptune_noaux_model.model, show_shapes=True).create(prog='dot', format='svg'))\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluating DeepTune wo. auxiliary inputs ...\", file=sys.stderr)\n",
    "deeptune_noaux = evaluate(DeepTuneNoAux())\n",
    "deeptune_noaux.groupby(['Platform', 'Benchmark Suite'])['Platform', 'Correct?', 'Speedup'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis of results. In the paper, we report that removing auxiliary inputs leads to a **12%** reduction in model accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speedup_loss = 1 - (deeptune_noaux['Speedup'].mean() / deeptune['Speedup'].mean())\n",
    "acc_loss = 1 - (deeptune_noaux['Correct?'].mean() / deeptune['Correct?'].mean())\n",
    "\n",
    "print(f\"Training without auxiliary inputs yields a \"\n",
    "      f\"{acc_loss:.0%} reduction in DeepTune's acccuracy, and a \"\n",
    "      f\"{speedup_loss:.0%} reduction in performance.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Seeding Transfer Learning\n",
    "\n",
    "Now we train a DeepTune model which we will re-use in Case Study B by transferring the learned weights: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Seeding transfer learning ... \", end=\"\")\n",
    "\n",
    "model_path = f\"../data/case-study-b/case-study-a.model\"\n",
    "weights_path = f\"../data/case-study-b/case-study-a-weights.h5\"\n",
    "\n",
    "if not fs.exists(weights_path):\n",
    "    deeptune_model = DeepTune()\n",
    "\n",
    "    if fs.exists(model_path):\n",
    "        deeptune_model = deeptune_model.restore(model_path)\n",
    "    else:\n",
    "        df = pd.read_csv(f\"../data/case-study-a/cgo17-amd.csv\")\n",
    "\n",
    "        sequences = encode_srcs(df[\"src\"].values)\n",
    "\n",
    "        features = grewe_features(df)\n",
    "        aux_in = auxiliary_inputs(df)\n",
    "\n",
    "        y = np.array([1 if x == \"GPU\" else 0 for x in df[\"oracle\"].values])\n",
    "        y_1hot = encode_1hot(y)\n",
    "\n",
    "        deeptune_model.init(seed=seed)\n",
    "        deeptune_model.train(df=df,\n",
    "                             features=features,\n",
    "                             aux_in=aux_in,\n",
    "                             sequences=sequences,\n",
    "                             y=y,\n",
    "                             y_1hot=y_1hot,\n",
    "                             verbose=True)\n",
    "        deeptune_model.save(model_path)\n",
    "\n",
    "    deeptune_model.model.save_weights(weights_path)\n",
    "\n",
    "print(\"done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See '*Case Study B.ipynb*' for experiments using transfer learning.\n",
    "\n",
    "End of experiments."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "104df134e61ee2e973e6704fd3055318865a4c9447c2680692bac6b44eefde8b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
