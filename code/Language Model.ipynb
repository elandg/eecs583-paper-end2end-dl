{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Demonstration\n",
    "\n",
    "### Source Rewriter\n",
    "\n",
    "Given the following example code (taken from Nvidiaâ€™s streamcluster benchmark):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code = \"\"\"//#define Elements\n",
    "__kernel void memset_kernel(__global char * mem_d, short val, int number_bytes){\n",
    "    const int thread_id = get_global_id(0);\n",
    "    mem_d[thread_id] = val;\n",
    "}\"\"\"\n",
    "\n",
    "print(code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply the rewriter. Variable and function names are normalized, comments removed, and code style enforced:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clgen import preprocess\n",
    "\n",
    "rewritten = preprocess(code)\n",
    "print(rewritten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source Encoder\n",
    "\n",
    "Deriving a 1-of-$k$ vocabulary for a piece of code, using a hybrid character and token based approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clgen._atomizer import GreedyAtomizer\n",
    "from clgen._langs import Language\n",
    "\n",
    "\n",
    "atomizer = GreedyAtomizer.from_text(lang=Language.from_str(\"opencl\"), text=rewritten)\n",
    "print(atomizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The derived vocabulary maps tokens to indices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(sorted([f\"'{k}'\" for k in atomizer.vocab]), columns=[\"token\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding the source using this vocabulary yields:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = atomizer.atomize(rewritten)\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reversing the process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in encoded:\n",
    "    t = atomizer.deatomize([i])\n",
    "    if t == '\\n': t = '\\\\n'\n",
    "    print(f\"<{t}>\", end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding\n",
    "\n",
    "Sequences are padded to a fixed length using an out-of-vocabulary token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "pad_val = atomizer.vocab_size\n",
    "print(pad_sequences([encoded], maxlen=len(encoded) + 22, value=pad_val)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2. Deriving Vocabulary from Handwritten GPGPU Benchmarks\n",
    "\n",
    "For the experiments in the paper, we derived a vocabulary from 45k lines of real world handwritten GPGPU benchmarks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "srcs = '\\n'.join(pd.read_csv(\"../data/case-study-a/cgo17-amd.csv\")['src'].values)\n",
    "print(\"lines of code:\", len(srcs.split('\\n')))\n",
    "\n",
    "derived_atomizer = GreedyAtomizer.from_text(lang=Language.from_str(\"opencl\"), text=srcs)\n",
    "print(\"derived vocabulary:\", derived_atomizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 10)\n",
    "pd.DataFrame(sorted([f\"'{k}'\" for k in derived_atomizer.vocab]), columns=[\"token\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this derived vocabulary, lets plot the first 80 tokens of 12 real world GPU benchmarks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "from labm8 import viz\n",
    "\n",
    "natoms = 80\n",
    "nprog = 12\n",
    "\n",
    "data = [derived_atomizer.atomize(src)[:natoms] for src in\n",
    "        pd.read_csv(\"../data/case-study-a/cgo17-amd.csv\")['src'].values[:nprog]]\n",
    "kernels = [x.split(\"-\")[-1] for x in\n",
    "           pd.read_csv(\"../data/case-study-a/cgo17-amd.csv\")['benchmark'].values[:nprog]]\n",
    "data = np.reshape(data, (nprog, natoms))\n",
    "    \n",
    "ax = sns.heatmap(data, vmin=0, vmax=derived_atomizer.vocab_size, square=True,\n",
    "                 cbar=False, yticklabels=kernels, xticklabels=[str(x) for x in range(1, natoms + 1)])\n",
    "plt.title(\"Encoded GPU benchmark sources (first 80 tokens)\")\n",
    "viz.finalise(figsize=(15, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that, as a result of the rewriting process, each of the kernels starts in an identical manner, i.e. `__kernel void A(...`\n",
    "\n",
    "# 3. Reproduce LaTeX Tables from the Paper\n",
    "\n",
    "Order the atoms by their appearance in the example code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i = 1\n",
    "s = set()\n",
    "ordered = []\n",
    "for idx in derived_atomizer.atomize(rewritten):\n",
    "    if idx not in s:\n",
    "        t = derived_atomizer.decoder[idx]\n",
    "        ordered.append((idx, t, i))\n",
    "        i += 1\n",
    "        s.add(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table 3c:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "\\\\footnotesize\n",
    "\\\\begin{tabular}{l l | l l | l l}\n",
    "  \\\\toprule\\\n",
    "\"\"\")\n",
    "print(\" \", \" & \".join([\"\\\\textbf{idx} & \\\\textbf{token}\"] * 3), \"\\\\\\\\\")\n",
    "print(\"  \\\\midrule\")\n",
    "\n",
    "def escape(x):\n",
    "    import re\n",
    "    if x == '\\n':\n",
    "        return '\\\\textbackslash n'\n",
    "    elif x == '{':\n",
    "        return '\\\\{'\n",
    "    elif x == '}':\n",
    "        return '\\\\}'\n",
    "    else:\n",
    "        return re.sub(r'_', '\\\\_', x)\n",
    "\n",
    "for i, (j, k, l) in enumerate(zip(ordered[:9], ordered[9:18], ordered[18:])):\n",
    "    i1, i2, i3 = i + 1, i + 10, i + 19\n",
    "    c1, c2, c3 = escape(j[1]), escape(k[1]), escape(l[1])\n",
    "    print(f\"\"\"  \\\n",
    "\\\\texttt{{{i1}}} & \\\\texttt{{`{c1}'}} & \\\n",
    "\\\\texttt{{{i2}}} & \\\\texttt{{`{c2}'}} & \\\n",
    "\\\\texttt{{{i3}}} & \\\\texttt{{`{c3}'}} \\\\\\\\\\\n",
    "\"\"\")\n",
    "    \n",
    "print(\"\"\"\\\n",
    "  \\\\bottomrule\n",
    "\\\\end{tabular}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table 3d:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = dict((i1, i2) for i1, _, i2 in ordered)\n",
    "\n",
    "print(\"\"\"\\\n",
    "\\\\rowcolors{2}{white}{gray!25}\n",
    "\\\\footnotesize\n",
    "\\\\begin{tabular}{l l l l l l l l l l l}\n",
    "  \\\\toprule\n",
    "  \"\"\", end=\"\")\n",
    "\n",
    "for i, idx in enumerate(derived_atomizer.atomize(rewritten)):\n",
    "    t = translator[idx]\n",
    "    print(f\"\\\\texttt{{{t:02d}}}\", end=\"\")\n",
    "    if (i + 1) % 11:\n",
    "        print(\" & \", end=\"\")\n",
    "    else:\n",
    "        print(\" \\\\\\\\\\n  \", end=\"\")\n",
    "\n",
    "print(\"\"\"\\\n",
    "\\\\multicolumn{2}{l}{\\\\texttt{<pad\\\\ldots}>} \\\\\\\\\"\"\")\n",
    "print(\"\"\"\\\n",
    "  \\\\bottomrule\n",
    "\\\\end{tabular}\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End of experiments."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "104df134e61ee2e973e6704fd3055318865a4c9447c2680692bac6b44eefde8b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
